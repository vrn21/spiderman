# Spiderman Web Crawler - Architecture

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SPIDERMAN WEB CRAWLER                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   Seed URL â”‚
                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        URL MANAGER                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ to_visit Queue:  [url1] â†’ [url2] â†’ [url3]              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ visited Set:     {url1, url2, url3, url4, url5}         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ get_next()
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Current URL  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          WEBSHOOTER                            â”‚
â”‚                      (HTTP Fetcher)                            â”‚
â”‚                                                                â”‚
â”‚  â€¢ Opens TCP connection                                        â”‚
â”‚  â€¢ Sends HTTP GET request                                      â”‚
â”‚  â€¢ Receives HTML response                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   HTML Contentâ”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                     â”‚
         â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LINK EXTRACTOR  â”‚  â”‚   HTML TO MD     â”‚
â”‚                  â”‚  â”‚    PARSER        â”‚
â”‚ â€¢ Find <a> tags  â”‚  â”‚                  â”‚
â”‚ â€¢ Extract hrefs  â”‚  â”‚ â€¢ Convert HTML   â”‚
â”‚ â€¢ Normalize URLs â”‚  â”‚ â€¢ Clean format   â”‚
â”‚ â€¢ Filter invalid â”‚  â”‚ â€¢ Extract text   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚
         â–¼                     â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  New URLs   â”‚      â”‚  Markdown   â”‚
  â”‚  [url4,     â”‚      â”‚  Content    â”‚
  â”‚   url5,     â”‚      â”‚             â”‚
  â”‚   url6]     â”‚      â”‚             â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚
         â”‚                     â–¼
         â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚           â”‚    DOCUMENT      â”‚
         â”‚           â”‚     MODEL        â”‚
         â”‚           â”‚  â€¢ url           â”‚
         â”‚           â”‚  â€¢ title         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  â€¢ content       â”‚â—€â”€â”€â”€â”
     add_url()       â”‚  â€¢ timestamp     â”‚    â”‚
                     â”‚  â€¢ links         â”‚    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                               â”‚             â”‚
                               â–¼             â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                     â”‚  EXPORT SYSTEM   â”‚    â”‚
                     â”‚                  â”‚    â”‚
                     â”‚ â€¢ Save to file   â”‚    â”‚
                     â”‚ â€¢ Save to DB     â”‚    â”‚
                     â”‚ â€¢ JSON/JSONL     â”‚    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                               â”‚             â”‚
                               â–¼             â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                     â”‚  SEARCH ENGINE   â”‚    â”‚
                     â”‚   (Your System)  â”‚    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                                             â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚  Loop continues
                     â”‚  until queue empty
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                             â”‚
                                             â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  CRAWL DONE    â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Details

### 1. URL Manager (âœ… Built)
- **Purpose**: Orchestrates the crawl
- **Input**: URLs to add
- **Output**: Next URL to crawl
- **Features**:
  - FIFO queue
  - Deduplication
  - Domain filtering
  - Max pages limit

### 2. Webshooter (âœ… Built)
- **Purpose**: Fetches web pages
- **Input**: URL string
- **Output**: HTML content
- **Features**:
  - Raw TCP connection
  - HTTP/1.1 protocol
  - User-Agent header

### 3. Link Extractor (âœ… Built)
- **Purpose**: Discovers new URLs
- **Input**: HTML + base URL
- **Output**: List of URLs
- **Features**:
  - Regex-based extraction
  - URL normalization
  - Invalid link filtering

### 4. HTML to MD Parser (âœ… Built)
- **Purpose**: Converts HTML to text
- **Input**: HTML string
- **Output**: Markdown string
- **Features**:
  - Clean text extraction
  - Format preservation
  - Whitespace normalization

### 5. Document Model (âŒ TODO)
- **Purpose**: Structured data format
- **Fields**:
  - url: String
  - title: String
  - content: String (markdown)
  - crawled_at: DateTime
  - links: Vec<String>

### 6. Export System (âŒ TODO)
- **Purpose**: Save crawled data
- **Formats**:
  - JSONL (recommended)
  - JSON
  - CSV
- **Destinations**:
  - File system
  - Database

## Data Flow Example

```
1. Start: seed_url = "http://example.com"
   â””â”€> URL Manager queue: [example.com]

2. Fetch: example.com
   â””â”€> Webshooter returns HTML

3. Process HTML:
   â”œâ”€> Link Extractor finds: [/about, /contact, /blog]
   â””â”€> HTML to MD produces: "# Example Domain\n\nThis is..."

4. Store:
   â”œâ”€> Create Document(url, title, content, timestamp, links)
   â””â”€> Export to file/database

5. Queue new URLs:
   â””â”€> URL Manager queue: [example.com/about, example.com/contact, example.com/blog]

6. Repeat from step 2 until queue empty
```

## Module Interaction

```
main.rs
  â”‚
  â””â”€> Spiderman::crawl()
        â”‚
        â”œâ”€> UrlManager::new(seed_url)
        â”‚     â”‚
        â”‚     â””â”€> Manages: to_visit queue, visited set
        â”‚
        â””â”€> Loop:
              â”‚
              â”œâ”€> UrlManager::get_next() â†’ current_url
              â”‚
              â”œâ”€> Spiderman::fetch(current_url)
              â”‚     â”‚
              â”‚     â””â”€> webshooter opens TCP, gets HTML
              â”‚
              â”œâ”€> extract_links(html, current_url) â†’ new_urls
              â”‚     â”‚
              â”‚     â””â”€> UrlManager::add_url(each new_url)
              â”‚
              â”œâ”€> parser(html) â†’ markdown
              â”‚
              â”œâ”€> Create Document
              â”‚
              â””â”€> Export Document â†’ file/db
```

## Crawl State Transitions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Start   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Initialize      â”‚
â”‚ URL Manager     â”‚
â”‚ with seed       â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Has next URL?   â”œâ”€â”€Noâ”€â”€â†’â”‚ Crawl Done   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ Yes
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Get next URL    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fetch HTML      â”œâ”€â”€Errâ”€â†’â”‚ Log error,   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ continue     â”‚
     â”‚ Ok                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extract links   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Add to queue    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parse to MD     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create Document â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Export Document â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Loop back
                    â–¼
```

## File Structure

```
spiderman/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ webshooter/          âœ… HTTP fetching
â”‚   â”‚   â”‚   â””â”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ html_to_md/          âœ… HTML to Markdown
â”‚   â”‚   â”‚   â””â”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ link_extractor/      âœ… Link discovery
â”‚   â”‚   â”‚   â””â”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ url_manager/         âœ… Queue management
â”‚   â”‚   â”‚   â””â”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ document/            âŒ TODO
â”‚   â”‚   â”‚   â””â”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ export/              âŒ TODO
â”‚   â”‚   â”‚   â””â”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ crawl.rs             âœ… Main crawl logic
â”‚   â”‚   â””â”€â”€ mod.rs               âœ… Module exports
â”‚   â””â”€â”€ main.rs                  âœ… Entry point
â”œâ”€â”€ Cargo.toml                   âœ… Dependencies
â”œâ”€â”€ MODULES_GUIDE.md             âœ… Detailed guide
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md    âœ… Quick summary
â””â”€â”€ ARCHITECTURE.md              âœ… This file
```

## Performance Characteristics

### Time Complexity
- URL Manager operations: O(1)
- Link extraction: O(n) where n = HTML size
- Deduplication: O(1) lookups

### Space Complexity
- URLs stored: O(m) where m = unique URLs found
- No HTML stored in memory (streaming)

### Scalability
- Can handle millions of URLs (limited by RAM for HashSet)
- Efficient queue operations
- No recursive stack (iterative)

## Next Steps for MVP

1. **Document Model** (10 min)
   - Create struct with url, title, content, timestamp, links
   
2. **Export System** (15 min)
   - Implement JSONL writer
   - Save documents to file
   
3. **Integration** (20 min)
   - Wire everything in crawl()
   - Add error handling
   
4. **Testing** (15 min)
   - End-to-end test
   - Verify output format

Total: ~60 minutes to MVP! ğŸš€
